{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basit LLM çağrısı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "OPENAI_API_KEY = \"AIzaSyCs05pZDzc8dyO_ZFtV63EtR-RWekElsFk\"\n",
    "\n",
    "# Chat modelini oluştur\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Kullanıcıdan giriş al\n",
    "user_input = \"LangChain nedir ve nasıl çalışır?\"\n",
    "\n",
    "# Prompt template oluştur\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Bana şunu basit bir dille açıkla: {question}\"\n",
    ")\n",
    "\n",
    "formatted_prompt = template.format(question=user_input)\n",
    "\n",
    "# Prompt'u mesaj formatında hazırlayın\n",
    "messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "\n",
    "# Yanıt al\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# Yanıtı yazdır\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basit prompt chaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain ,SequentialChain\n",
    "\n",
    "OPENAI_API_KEY = \"AIzaSyCs05pZDzc8dyO_ZFtV63EtR-RWekElsFk\"\n",
    "\n",
    "# Google Gemini Modelini Tanımla\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=OPENAI_API_KEY)\n",
    "# 1. İlk Zincir: Kişinin doğduğu şehri öğren\n",
    "prompt1 = ChatPromptTemplate.from_template(\"What is the city {person} is from?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"What country is the city {city} in? Respond in {language}\")\n",
    "\n",
    "# 2. Modeli Tanımla\n",
    "model = llm\n",
    "\n",
    "# 3. İlk LLM Zinciri (Kişinin doğduğu şehri bul)\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "# 4. İkinci LLM Zinciri (Şehri alıp, hangi ülkede olduğunu öğren)\n",
    "chain2 = (\n",
    "    prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 5. Zincirleri Birleştir\n",
    "final_chain = {\n",
    "    \"city\": chain1,\n",
    "    \"language\": itemgetter(\"language\")\n",
    "} | chain2\n",
    "\n",
    "# 6. Çalıştır ve Sonucu Yazdır\n",
    "result = final_chain.invoke({\"person\": \"obama\", \"language\": \"spanish\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1️⃣ LLM çağrıları yapmayı öğren (OpenAI, Hugging Face gibi).\n",
    "2️⃣ Prompt Template ile dinamik prompt oluşturmayı öğren.\n",
    "3️⃣ Prompt Chaining ile AI süreçlerini bağlamayı öğren.\n",
    "4️⃣ LangChain Memory, Agents, Tools gibi ileri seviye konseptlere geç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill mask app"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"google/mobilebert-uncased\",\n",
    "    tokenizer=\"google/mobilebert-uncased\"\n",
    ")\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "#print(fill_mask(f\"HuggingFace is creating a {fill_mask.tokenizer.mask_token} what is vs code\"))\n",
    "\n",
    "\n",
    "class JokeRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "@app.post(\"/test1\")\n",
    "async def test_endpoint(request: JokeRequest):\n",
    "\n",
    "    user_input = request.message\n",
    "\n",
    "    result =f\"{user_input}{fill_mask.tokenizer.mask_token}\"\n",
    "\n",
    "    response=fill_mask(result)\n",
    "\n",
    "    return {\"message\": response}\n",
    "\n",
    "\n",
    "@app.get(\"/test\")\n",
    "async def test_endpoint():\n",
    "    return {\"message\": \"This is a test string!\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question app (gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatAnthropic, ChatOpenAI\n",
    "from langserve import add_routes\n",
    "\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "dotenv_path = Path('key.env')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "\n",
    "# Chat modelini oluştur\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=OPENAI_API_KEY)\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"A simple api server using Langchain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "template = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"Bunu bana karadeniz şivesiyle söyle: {question}\"\n",
    "    )\n",
    "\n",
    "class JokeRequest(BaseModel):\n",
    "    topic: str\n",
    "\n",
    "@app.post(\"/joke\")\n",
    "async def joke_endpoint(request: JokeRequest):\n",
    "    prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "\n",
    "    user_input = \"tell me a simple joke?\"\n",
    "\n",
    "    # Prompt template oluştur\n",
    "\n",
    "\n",
    "    formatted_prompt = template.format(question=user_input)\n",
    "\n",
    "    # Prompt'u mesaj formatında hazırlayın\n",
    "    messages = [{\"role\": \"user\", \"content\": formatted_prompt}]\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"joke\": response}\n",
    "\n",
    "\n",
    "@app.get(\"/test\")\n",
    "async def test_endpoint():\n",
    "    return {\"message\": \"This is a test string!\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrewAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crew ai başla   https://docs.crewai.com/quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph = onenote da projenin graph çıkarılşabilir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " LangSmith bir projeyi incele\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
